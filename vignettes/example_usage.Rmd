---
title: "Example usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE, cache=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, cache=TRUE}
library(tidyprompt)
```

### Setup an LLM provider

`tidyprompt` can be used with any LLM provider capable of completing a chat. 

At the moment, `tidyprompt` includes pre-built functions to connect with Ollama and the OpenAI API. 

With the `create_llm_provider` function, you can easily write a hook for any other LLM provider. 
You could make API calls using the `httr` package or use another R package that already has 
a hook for the LLM provider you want to use.

```{r, cache=TRUE}
# Ollama running on local PC
ollama <- create_ollama_llm_provider(
  parameters = list(model = "llama3.1:8b", url = "http://localhost:11434/api/chat")
)

# OpenAI API
openai <- create_openai_llm_provider(
  parameters = list(model = "gpt-4o-mini", api_key = Sys.getenv("OPENAI_API_KEY"))
)

# Build a hook to your own LLM provider with create_llm_provider; 
#   see the documentation for more information; you can also look
#   at the source code of create_ollama_llm_provider and create_openai_llm_provider

# Example usage
response <- ollama$complete_chat("Hello there!")
print(response$content)
```

### Basic prompting

A simple string serves as the base for a prompt. 

By adding prompt wrappers, you can influence various aspects of how the LLM handles the prompt, 
while verifying that the output is structured and valid (including retries with feedback to the LLM if it is not).

```{r, cache=TRUE}
  "Hi there!" |>
    send_prompt(ollama, verbose = FALSE)

```
`add_text` is a simple example of a prompt wrapper. It simply adds some text at the end of the base prompt.

```{r, cache=TRUE}
  "Hi there!" |>
    add_text("What is a large language model? Explain in 10 words.") |>
    send_prompt(ollama, verbose = FALSE)
```
You can also construct the final prompt text, without sending it to an LLM provider.

```{r, cache=TRUE}
  "Hi there!" |>
    add_text("What is a large language model? Explain in 10 words.") |>
    construct_prompt_text() |>
    cat()
```

### Retrieving output in a specific format

Using prompt wrappers, you can force the LLM to return the output in a specific format.
You can also extract the output to turn it from a character into another data type.

For instance, the `answer_as_integer` prompt wrapper will force the LLM to return an integer.

To achieve this, the prompt wrapper will add some text to the base prompt, asking
the LLM to reply with an integer. However, the prompt wrapper does more: it also
will attempt to extract and validate the integer from the LLM's response. If
extraction or validation fails, feedback is sent back to the LLM, after which 
the LLM can retry answering the prompt.

```{r, cache=TRUE}
  "What is 2 + 2?" |>
    answer_as_integer(add_instruction_to_prompt = TRUE) |>
    send_prompt(ollama, verbose = TRUE)
```
Below is an example of a prompt which will initially fail, but will succeed after a retry.

```{r, cache=TRUE}
  "What is 2 + 2?" |>
    add_text("Please write out your reply in words, use no numbers.") |>
    answer_as_integer(add_instruction_to_prompt = FALSE) |>
    send_prompt(ollama, verbose = TRUE)
```

### Adding a reasoning mode to the LLM

Prompt wrappers may also be used to add a reasoning mode to the LLM. This may
improve the LLM's performance on more complex tasks. 

For instance, function `set_mode_chainofthought` will add chain of thought reasoning mode to the LLM.
This wraps the base prompt within a request for the LLM to reason step by step, asking
it to provide the final answer within 'FINISH[<final answer here>]'. An extraction
function then ensures only the final answer is returned.

```{r, cache=TRUE}
  "What is 2 + 2?" |>
    set_mode_chainofthought() |>
    answer_as_integer() |>
    send_prompt(ollama, verbose = TRUE)
```

### Giving tools to the LLM (autonomous function-calling)

With `tidyprompt`, you can define R functions and give the LLM the ability to call them.
This enables the LLM to retrieve additional information or take other actions.

```{r, cache=TRUE}
  # Define a function that returns fake data about the temperature in a location
  temperature_in_location <- function(
    location = c("Amsterdam", "Utrecht", "Enschede"),
    unit = c("Celcius", "Fahrenheit")
  ) {
    #' llm_tool::name temperature_in_location
    #'
    #' llm_tool::description Get the temperature in a location
    #'
    #' llm_tool::param location Location, must be one of: "Amsterdam", "Utrecht", "Enschede"
    #' llm_tool::param unit Unit, must be one of: "Celcius", "Fahrenheit"
    #'
    #' llm_tool::return The temperature in the specified location and unit
    #'
    #' llm_tool::example
    #' temperature_in_location("Amsterdam", "Fahrenheit")
    
    # As shown above, one can use docstring-like text to document the function.
    #   This will provide the LLM information on what the function does,
    #   and how it should be used.
    
    location <- match.arg(location)
    unit <- match.arg(unit)

    temperature_celcius <- switch(
      location,
      "Amsterdam" = 32.5,
      "Utrecht" = 19.8,
      "Enschede" = 22.7
    )

    if (unit == "Celcius") {
      return(temperature_celcius)
    } else {
      return(temperature_celcius * 9/5 + 32)
    }
  }

  # Ask the LLM a question which can be answered with the function
  prompt <- "Hi, what is the weather temperature in Enschede?" |>
    add_text("I want to know the Celcius degrees.") |>
    answer_as_integer() |>
    add_tools(temperature_in_location) |>
    send_prompt(ollama, verbose = TRUE)
```
### Creating and using your own prompt wrappers

Under the hood, prompts are just lists of a base prompt (a string) and a series of prompt wrappers.

You can thus create a function which takes a prompt and appends a new prompt wrapper to it.

Take a look at the source code for function `add_text`:

```{r echo=TRUE, eval=FALSE}
add_text <- function(prompt_wrap_or_list, text, sep = "\n\n") {
  prompt_list <- validate_prompt_list(prompt_wrap_or_list)

  new_wrap <- create_prompt_wrap(
    modify_fn = function(original_prompt_text, modify_fn_args) {
      text <- modify_fn_args$text
      sep <- modify_fn_args$sep
      return(paste(original_prompt_text, text, sep = sep))
    },
    modify_fn_args = list(text = text, sep = sep)
  )

  return(c(prompt_list, list(new_wrap)))
}
```

More complex may also add extraction and validation functions. 

They key difference between an extraction and validation function is that an 
extraction function alters the LLM's response and passes on the altered response
to next extraction and/or validation functions, and eventually to the
return statement of send_prompt (if extractions and validations are succesful).
A validation function, on the other hand, only checks if the LLM's response passes a logical test. Both
extraction and validation functions can return feedback to the LLM.

For more information, on what you can do with prompt wrappers, see the documentation
of the `prompt_wrap` class creator function: `create_prompt_wrap`. For examples of prompt wrapper functions, 
see, for instance the documentation and source code of `add_text`, `answer_as_integer`, 
`set_mode_chainofthought`, and `add_tools`.
