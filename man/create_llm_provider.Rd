% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_provider.R
\name{create_llm_provider}
\alias{create_llm_provider}
\title{Generic function to assist in creating llm_provider objects}
\usage{
create_llm_provider(complete_chat_function, parameters = list())
}
\arguments{
\item{complete_chat_function}{Function that will be called by the llm_provider
to complete a chat.

This function should take a chat_history dataframe as input (see ?validate_chat_history),
and return a list of 'role' and 'content' for the next message (e.g., list(role = "user", content = "Hello")).

An llm_provider object will wrap the provided complete_chat_function with a validation of chat_history,
also turning a single string into a valid chat_history dataframe.
The provided function thus does not need to do this but should assume that
the input is a valid chat_history dataframe.

Parameters passed in the parameters argument may be accessed by the complete_chat_function;
this may be used to, for instance, store an API key, the name of the model to use, or other settings.}

\item{parameters}{A named list of parameters that will be attached to the
llm_provider object. These parameters can be used to configure the llm_provider.
For instance, they can be used to store a model's name, an API key, or an endpoint.
E.g., list(model = "my-llm-model", api_key = "my-api-key").}
}
\value{
A new llm_provider object
}
\description{
This function can be used to create new llm_provider objects with different
implementations of the complete_chat function.
}
\examples{
# Below is an example of how to create a custom llm_provider using this function
# (See also the source code for tidyprompt::create_openai_llm_provider
#    and tidyprompt::create_ollama_llm_provider)

# First create a wrapper around the generic function,
#  providing your implementation to complete a chat
create_custom_llm_provider <- function(parameters = list()) {
 # Define a function to complete a chat
 complete_chat <- function(chat_history) {
   # Your implementation to complete a chat here, e.g., call an API
   call_my_fake_api <- function(chat_history) {
     message("Calling my fake API...")
     message("Using fake model name: ", parameters$model)
     message("Using fake API key: ", parameters$api_key)
     message("And we are not passing chat_history to the fake API, but you should!")

     return(list(role = "assistant", content = "Hi, this is a fake API response!"))
   }

   api_response <- call_my_fake_api(chat_history)
   return(list(role = api_response$role, content = api_response$content))
 }

 # Create the llm_provider object using the generic function
 create_llm_provider(
   complete_chat_function = complete_chat,
   parameters = parameters
 )
}

# Then create and use an instance, like so:
custom_llm <- create_custom_llm_provider(list(model = "my-llm-model", api_key = "my-api-key"))
custom_llm$complete_chat("Hello!")
custom_llm$set_parameters(list(model = "my-new-llm-model"))
custom_llm$get_parameters()
}
